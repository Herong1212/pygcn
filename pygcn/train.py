from __future__ import division
from __future__ import print_function

import time
import argparse
import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim

from pygcn.utils import load_data, accuracy
from pygcn.models import GCN

# step1ï¼šTraining settings
parser = argparse.ArgumentParser()  # è®¾ç½®è¶…å‚æ•°ï¼Œå¦‚æ˜¯å¦ä½¿ç”¨ CUDAã€å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ç­‰
# è®¾ç½®æ˜¯å¦ç¦ç”¨ CUDA
parser.add_argument(
    "--cuda", action="store_true", default=True, help="Enables CUDA training."
)
# è®¾ç½®éšæœºç§å­ã€è®­ç»ƒè½®æ•°ã€å­¦ä¹ ç‡ç­‰è¶…å‚æ•°
parser.add_argument(
    "--fastmode",
    action="store_true",
    default=False,
    help="Validate during training pass.",
)
parser.add_argument("--seed", type=int, default=42, help="Random seed.")
parser.add_argument(
    "--epochs", type=int, default=200, help="Number of epochs to train."
)
parser.add_argument("--lr", type=float, default=0.01, help="Initial learning rate.")
parser.add_argument(
    "--weight_decay",
    type=float,
    default=5e-4,
    help="Weight decay (L2 loss on parameters).",
)
parser.add_argument("--hidden", type=int, default=16, help="Number of hidden units.")
parser.add_argument(
    "--dropout", type=float, default=0.5, help="Dropout rate (1 - keep probability)."
)

args = parser.parse_args()
args.cuda = args.cuda and torch.cuda.is_available()
print(args.cuda)
print(torch.cuda.is_available())
# è¾“å‡º CUDA çŠ¶æ€
if args.cuda:
    print("CUDA is enabled.")
else:
    print("CUDA is disabled.")

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯é‡å¤æ€§
np.random.seed(args.seed)
torch.manual_seed(args.seed)
# å¦‚æœä½¿ç”¨ GPUï¼Œè¿˜éœ€è¦ç”¨ torch.cuda.manual_seed() æ¥å›ºå®š GPU çš„éšæœºæ€§
if args.cuda:
    print(args.cuda)
    torch.cuda.manual_seed(args.seed)

# step2ï¼šLoad data
# ? idxæ˜¯æŒ‡æ¯ä¸ªèŠ‚ç‚¹çš„ç´¢å¼•å—ï¼Ÿå°±æ˜¯æ¯ä¸€ç¯‡æ–‡çŒ®ï¼Ÿ
adj, features, labels, idx_train, idx_val, idx_test = load_data()

# step3ï¼šModel and optimizer
# psï¼šè®¾ç½®æ¨¡å‹å‚æ•°
# ? features.shape[1]å•¥æ„æ€ï¼Ÿlabels.max().item() + 1 å•¥æ„æ€ï¼Ÿæ¯ä¸ªæ•°æ®å„æ˜¯ä»€ä¹ˆæ•°æ®ç±»å‹ï¼Ÿ
model = GCN(
    nfeat=features.shape[1],  # è¾“å…¥èŠ‚ç‚¹çš„ç‰¹å¾ç»´åº¦ï¼ˆå³ç‰¹å¾çŸ©é˜µçš„åˆ—æ•°ï¼‰
    nhid=args.hidden,  # éšè—å±‚çš„ç¥ç»å…ƒæ•°é‡
    nclass=labels.max().item() + 1,  # åˆ†ç±»æ•°ï¼ˆæ ‡ç­¾çš„æœ€å¤§å€¼ + 1ï¼‰
    dropout=args.dropout,  # Dropout æ¯”ä¾‹
)
# psï¼šè®¾ç½®ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œè®¾ç½®å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡
optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

# å°†æ¨¡å‹å’Œæ•°æ®è¿ç§»åˆ° GPU ä¸Šè¿è¡Œï¼Œä»¥åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹
print(args.cuda)
if args.cuda:
    print("Migrate the model and data to run on the GPU...")
    model.cuda()
    features = features.cuda()
    adj = adj.cuda()
    labels = labels.cuda()
    idx_train = idx_train.cuda()
    idx_val = idx_val.cuda()
    idx_test = idx_test.cuda()
    print("Migration is over!")

print(f"Model device: {next(model.parameters()).device}")
print(f"Features device: {features.device}")
print(f"Adjacency matrix device: {adj.device}")
print(f"Labels device: {labels.device}")
print(f"idx_train device: {idx_train.device}")
print(f"idx_val device: {idx_val.device}")
print(f"idx_test device: {idx_test.device}")


# step4ï¼šè®­ç»ƒ
def train(epoch):
    t = time.time()

    # åˆ‡æ¢æ¨¡å‹åˆ°ã€è®­ç»ƒæ¨¡å¼ã€‘
    model.train()
    # æ¸…ç©ºæ¢¯åº¦
    optimizer.zero_grad()
    # 1ã€å‰å‘ä¼ æ’­
    output = model(features, adj)
    # 2ã€è®¡ç®—è®­ç»ƒé›†çš„ã€è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‘
    loss_train = F.nll_loss(output[idx_train], labels[idx_train])
    # 3ã€è®¡ç®—è®­ç»ƒé›†çš„å‡†ç¡®ç‡
    acc_train = accuracy(output[idx_train], labels[idx_train])
    # 4ã€åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    loss_train.backward()
    # 5ã€æ›´æ–°æ¨¡å‹å‚æ•°
    optimizer.step()

    if not args.fastmode:
        # Evaluate validation set performance separately, deactivates dropout during validation run.
        # åˆ‡æ¢æ¨¡å‹åˆ°ã€éªŒè¯æ¨¡å¼ã€‘
        model.eval()
        # å†æ¬¡è®¡ç®—è¾“å‡ºï¼ˆå…³é—­ Dropoutï¼‰
        output = model(features, adj)

    # 6ã€éªŒè¯æ¨¡å¼ä¸‹é‡æ–°è®¡ç®—è¾“å‡ºï¼Œå¹¶è®°å½•éªŒè¯é›†çš„æŸå¤±å’Œå‡†ç¡®ç‡
    # éªŒè¯é›†æŸå¤±
    loss_val = F.nll_loss(output[idx_val], labels[idx_val])
    # éªŒè¯é›†å‡†ç¡®ç‡
    acc_val = accuracy(output[idx_val], labels[idx_val])
    print(
        "Epoch: {:04d}".format(epoch + 1),
        "loss_train: {:.4f}".format(loss_train.item()),
        "acc_train: {:.4f}".format(acc_train.item()),
        "loss_val: {:.4f}".format(loss_val.item()),
        "acc_val: {:.4f}".format(acc_val.item()),
        "time: {:.4f}s".format(time.time() - t),
    )


# step5ï¼šæµ‹è¯•
def test():
    # åˆ‡æ¢ä¸ºã€è¯„ä¼°æ¨¡å¼ã€‘
    model.eval()
    output = model(features, adj)
    loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])
    print(
        "Test set results:",
        "loss= {:.4f}".format(loss_test.item()),
        "accuracy= {:.4f}".format(acc_test.item()),
    )


# Train model
t_total = time.time()
for epoch in range(args.epochs):
    train(epoch)

# step6ï¼šä¿å­˜æ¨¡å‹ï¼ˆåœ¨è®­ç»ƒå®Œæˆåä¿å­˜ï¼‰ğŸ‘‡
# ä¿å­˜æ¨¡å‹ï¼ˆåŒ…æ‹¬æ¨¡å‹çš„ç»“æ„å’Œå‚æ•°ï¼‰
torch.save(model, "../checkpoints/model.pt")
# æˆ–è€…åªä¿å­˜æ¨¡å‹çš„å‚æ•°ï¼ˆæƒé‡ï¼‰
# torch.save(model.state_dict(), f"gcn_model_epoch_{epoch+1}.pth")

print("Optimization Finished!")
print("Total time elapsed: {:.4f}s".format(time.time() - t_total))

# Testing
test()
